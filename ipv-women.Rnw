\documentclass[10pt,english]{article}
\usepackage[paperwidth=164mm, paperheight=280mm, top=19mm, bottom=19mm, left=1cm, right=1cm]{geometry} % 16:9 three pages in a row
\usepackage[english]{babel}
\usepackage{longtable}
\usepackage{fontspec}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{varioref} %% vref
\usepackage{paralist} %% compactenum
\usepackage{siunitx} %% \num
\usepackage{IEEEtrantools} % proper indentation for multi-line equations
\usepackage{amsmath} %% \pmatrix
\usepackage{subfig}
\usepackage{apacite}

%% make a single « start and end a chunk of \texttt{}, and two «« to represent the literal «.
\catcode`«=\active
\def«#1«{\texttt{#1}}

\begin{document}

% This chunk is needed to make sure evaluation is taking place in the current directory. The other settings are nice but not as important.
<<setup.outer, echo=FALSE, tidy=FALSE, message=FALSE>>=
library(knitr)
opts_chunk$set(results='hide', cache=TRUE, echo=TRUE, warning=TRUE, fig.width=4, fig.height=4, fig.pos = 'htb', tidy.opts=list(blank=FALSE, width.cutoff=50), background='white', tidy=TRUE, error=TRUE, message=FALSE, dev="png", dpi = 200, autodep = TRUE)
opts_knit$set(root.dir = ".")
options(scipen=999)
@

<<setup.dir, echo=FALSE, tidy=FALSE, message=FALSE>>=
## SNIC
if(file.exists("/mimer/NOBACKUP/groups/globalpoverty1/hans")){
    my.dir <- "/mimer/NOBACKUP/groups/globalpoverty1/hans/global-living-conditions"
    ## "africa-lowres.RData"
    africa.map.path <- "~/"
    my.max.file.size.for.parallelisation = 9000000000
    ## "GDL-Subnational-GDI-data.csv", "GDL Shapefiles V6.4"
    gdl.path <- "/mimer/NOBACKUP/groups/globalpoverty1/hans/global-living-conditions/"
    path.to.python <- "/usr/bin/python3"
}
## 245
if(file.exists("/media/sf_Our_data/Data/DHS")){
    my.dir <- "/media/sf_Our_data/Data/DHS"
    africa.map.path <- "~/annex/projekt/child-poverty/statiska-eller-stora-filer/"
    my.max.file.size.for.parallelisation = 2000000000
    gdl.path <- "~/annex/projekt/global-living-conditions/stora-eller-statiska-filer/"  
}
if(Sys.info()["sysname"] == "Windows"){
    path.to.python <- "C:/Users/hanse/miniconda3/python.exe"
} 

@

\section{Install software}
To compile this document, including rendering the graphs, a working «R« and «Python/PYMC« installation is assumed. If run a on High-performance computing (HPC) cluster with restrictions on software installation that inhibits on the fly installation of needed «R« and «Python« packages, make sure the following packages are installed.

<<install.packages>>=
## Install necessary R packages
required.packages <- c("lme4", "data.table", "xtable", "effects", "parallel", "future", "doFuture", "foreach", "sf", "terra", "parallelly", "cooltools", "raster", "formatR", "lwgeom", "autoFRK", "rnaturalearthdata", "rnaturalearth", "spatstat.geom", "spatstat.random")
for(package in required.packages){
    if(!require(package, character.only=TRUE)){
        install.packages(package, character.only=TRUE)
    }
}
@

The following libraries are required: numpy, pandas, pickle, random, os, jax, pymc, arviz, patsy.

\section{Prepare data}
Download and standardize data with the R package «globallivingconditions«. Construct the variables «woman.exposed.to.IPV«, «woman.thinks.IPV.can.be.justified« and «man.thinks.\-IPV.can.be.justified«. Based on «woman.thinks.IPV.can.be.justified« and the sampling structure, create «justifyIPV\_cluster«. In a similar vein, construct the variable «viol01\_cluster« based on «woman.exposed.to.IPV«.

Create the following variables: «beat01\_couple« (attitude to IPV), «iliterate\_couple«, «decide2\_health« (how are decisions made about health care for the woman), «decide2\_purchases« (how are decisions made about large purchase), «edudeviation« (difference in education in the couple) , «iwi2« (wealth level of the household), «lnage\_wife« (the age of the wife, logarithm), «ln\_numb\_of\_children« (number of children, logarithm), «gdi\_s« (gender index at the regional level), «gni\_s« (estimated gross national income at the regional level).

«beat01\_couple« is a classification of the couples into one of the four possible combinations of attitudes to IPV by the wife and the husband in the couple:

\begin{compactenum}
\item No one accepts IPV 
\item Wife accepts IPV 
\item Husband accepts IPV 
\item Both accept IPV 
\end{compactenum}

It is based on the following five questions, encoded in 10 variables (5 for the husband and 5 for the man), based on the the following statment:

``In your opinion, is a husband justified in hitting or beating his wife in the following situations:''

The table below show which variable encodes the answer from the man and the woman respectively for each hypothetical situation.
\begin{table}
  \begin{centering}
  \begin{tabular}{lll}
    \toprule
    Question & Women & Men \\
    \midrule
    If she goes out without telling him? & V744A & MV744A \\
    If she neglects the children? & V744B & MV744B \\
    If she argues with him? & V744C & MV744C \\
    If she refuses to have sex with him? & V744D & MV744D \\
    If she burns the food? & V744E & MV744E \\
    \bottomrule
  \end{tabular}
  \caption{The scenarios used to query both women and men about if IPV can be justified.}
\end{centering}
\end{table}


\begin{description}
\item[«beat01\_couple«] Match couples via «V034« for women and «PersonID.unique« for men, four different levels representing the four possible combinations of two boolean variables.
\item[«decide2\_health«] Encode how decisions on health care for the wife are made based on «(M)V744A«. Combine all sorts of disagreements into one level. Discard couples that agree on the very unusual option: ``Some else decides'' (only recorded in 1 in 1000 couples for health care for the wife).
\item[«decide2\_purchases«] Same as for health decisions but based on «(M)V744B«
\item[«edudeviation«] The education level of the husband «(MV149)« compared to the educational level of the wife.

\end{description}

Also add GDI and HDI subnational data from the \href{Global Data Lab}{https://globaldatalab.org/shdi/table/sgdi/} (For now, use manually downloaded local CSV-files and map our GIS points to their Shapefiles, but when their package is fixed, use that instead of local files).

<<björns.set, echo = FALSE>>=
## Not in björns set
## Bangladesh, Burundi, Chad, Congo, Democratic Republic of the Congo, Egypt, Guatemala, Guinea, Jordan = "81", Kenya = "8C", Lesotho, Mozambique = "81", Namibia, Philippines, Senegal, Tajikistan, Timor-Leste, Turkey, Uganda

## no data about violence in the set above
## Bangladesh, Congo, Guinea, Turkey, 

## Not in my set
## Jordan "74", Kenya = "72", Mali = "83", Mozambique = "62", Nepal = "7H", Tanzania = "7B", Zambia = "61"

## ## my.set
## waves = list("Afghanistan" = "71", "Angola" = "71", "Armenia" = "72", "Bangladesh" = "81", "Benin" = "71", "Burkina Faso" = "81", "Burundi" = "71",
##                  "Cambodia" = c("73", "82"), "Cameroon" = "71", "Chad" = "71", "Colombia" = c("71", "72"), "Congo" = "61",
##                  "Congo Democratic Republic" = "61", "Cote d'Ivoire" = "81", "Dominican Republic" = "61", "Egypt" = "61", "Ethiopia" = "71", "Haiti" = "71",
##                  "Gabon" = "71", "Gambia" = "81", "Ghana" = "8C", "Guatemala" = "71", "Guinea" = "71", "India" = c("74", "7E"),
##                  "Jordan" = "81", "Kenya" = "8C", "Lesotho" = "81", "Liberia" = "7A", "Madagascar" = "81", "Maldives" = "71", "Malawi" = "7A", 
##                  "Mali" = "7A", "Mauritania" = "71", "Mozambique" = "81", "Myanmar" = "71", "Namibia" = "61", "Nepal" = "82",
##                  "Nigeria" = "7B", "Pakistan" = "71", "Papua New Guinea" = "71", "Philippines" = "82", "Rwanda" = "81", "Senegal" = "8B",
##                  "Sierra Leone" = "7A", "South Africa" = "71", "Tajikistan" = "72", "Tanzania" = "82", "Timor-Leste" = "71", "Togo" = "61",
##              "Turkey" = "71", "Uganda" = "7B", "Zambia" = "71", "Zimbabwe" = "72"),

## ## björns set

## waves = list("Afghanistan" = "71", "Angola" = "71", "Armenia" = "72", "Benin" = "71", "Burkina Faso" = "81", 
##                  "Cambodia" = c("73", "82"), "Cameroon" = "71", "Colombia" = "72",
##                  "Cote d'Ivoire" = "81", "Dominican Republic" = "61", "Ethiopia" = "71", "Haiti" = "71",
##                  "Gabon" = "71", "Gambia" = "81", "Ghana" = "8C", "Guinea" = "71", "India" = c("74", "7E"),
##                  "Jordan" = "74", "Kenya" = "72", "Liberia" = "7A", "Madagascar" = "81", "Maldives" = "71", "Malawi" = "7A", 
##                  "Mali" = "7A", "Mauritania" = "71", "Mozambique" = "62", "Myanmar" = "71", "Nepal" = "7H",
##                  "Nigeria" = "7B", "Pakistan" = "71", "Papua New Guinea" = "71", "Rwanda" = "81", 
##                  "Sierra Leone" = "7A", "South Africa" = "71", "Tanzania" = "7B", "Togo" = "61", "Zambia" = "61", "Zimbabwe" = "72"),

@ 

<<load.data.from.scratch>>=
library(devtools)
install_bitbucket(repo = "hansekbrand/iwi", upgrade = "never")
install_bitbucket(repo = "hansekbrand/DHSharmonisation", ref="debug", upgrade = "never")
library(globallivingconditions)
load("~/user_credentials_dhs.RData") # a list with two named elements:
                                   # "dhs.user" and "dhs.password" 
my.dt <- download.and.harmonise(
    dhs.user=credentials$dhs.user,
    dhs.password=credentials$dhs.password,
    waves = list("Afghanistan" = "71", "Angola" = "71", "Armenia" = "72", "Benin" = "71", "Burkina Faso" = "81", 
                 "Cambodia" = c("73", "82"), "Cameroon" = "71", "Colombia" = "72",
                 "Cote d'Ivoire" = "81", "Dominican Republic" = "61", "Ethiopia" = "71", "Haiti" = "71",
                 "Gabon" = "71", "Gambia" = "81", "Ghana" = "8C", "Guinea" = "71", "India" = c("74", "7E"),
                 "Jordan" = "74", "Kenya" = "72", "Liberia" = "7A", "Madagascar" = "81", "Maldives" = "71", "Malawi" = "7A", 
                 "Mali" = "7A", "Mauritania" = "71", "Mozambique" = "62", "Myanmar" = "71", "Nepal" = "7H",
                 "Nigeria" = "7B", "Pakistan" = "71", "Papua New Guinea" = "71", "Rwanda" = "81", 
                 "Sierra Leone" = "7A", "South Africa" = "71", "Tanzania" = "7B", "Togo" = "61", "Zambia" = "61", "Zimbabwe" = "72"),
    vars.to.keep = NULL,
    file.types.to.download = c("PR", "GE", "IR", "MR", "KR"),
    variable.packages = c("wealth", "empowerment", "matching"),
    max.file.size.for.parallelisation = my.max.file.size.for.parallelisation,
    directory = my.dir,
    check.dhs.for.more.data = FALSE
)

## Domestic Violence
my.dt[ , woman.exposed.to.IPV := apply(my.dt[, list(empowerment.D106, empowerment.D107, empowerment.D108), ], 1, any), ]
my.dt[ , woman.thinks.IPV.can.be.justified := apply(my.dt[, list(empowerment.V744A, empowerment.V744B, empowerment.V744C, empowerment.V744D, empowerment.V744E), ], 1, any), ]
my.dt[ , man.thinks.IPV.can.be.justified := apply(my.dt[, list(empowerment.MV744A, empowerment.MV744B, empowerment.MV744C, empowerment.MV744D, empowerment.MV744E), ], 1, any), ]

## Keep only cases with data on at least one of our focal variables
my.dt <- my.dt[which(is.na(woman.exposed.to.IPV) == FALSE |
                     is.na(woman.thinks.IPV.can.be.justified) == FALSE |
                     is.na(man.thinks.IPV.can.be.justified) == FALSE), ]

save(my.dt, file = "temp.RData")

contextuals.f <- function(var.name, my.dt){
    ## make var.name a binary variable
    my.dt$source.var <- ifelse(my.dt[[var.name]] == 1, TRUE, FALSE)

    n.true.by.cluster <- my.dt[, list(n.true = length(which(source.var))), by = ClusterID]
    n.no.na.by.cluster <- my.dt[, list(total.n = length(which(! is.na(source.var)))), by = ClusterID]
    my.dt.2 <- merge(n.no.na.by.cluster, n.true.by.cluster)
    ## what to do with the cluster with only a single observation? Are these observations part of a cluster at all?
    ## discard them
    ## my.dt.2 <- my.dt.1[total.n > 1, ]
    ## there are three cases: 0; 0 < n.true < total.n; n.true == total.n
    ## only in the middle case, we need to calculate something
    these.have.zero.true.cases <- which(my.dt.2$n.true == 0)
    these.have.only.true.cases <- which(my.dt.2$n.true == my.dt.2$total.n)
    clusters.with.disagreement <- my.dt.2[which(! (my.dt.2$n.true == 0 | (my.dt.2$n.true == my.dt.2$total.n))), ]
    
    ## head(clusters.with.disagreement)
    ## Key: <ClusterID>
    ##                                  ClusterID total.n n.true
    ##                                     <fctr>   <int>  <int>
    ## 1: IA.Andaman And Nicobar Islands.74.10082       8      2
    ## 2: IA.Andaman And Nicobar Islands.74.10090       8      1
    ## 3: IA.Andaman And Nicobar Islands.74.10183       8      2
    
    ## the two possible manipulations are:
    ## decrease total.n by 1
    ## decrease n.true by 1 and total.n by 1
    
    ## save two dataset, one where total.n is decreased by one (ego had FALSE, so removing ego from the total did not change the n.true)
    ## the other one with the double manipulation.
    
    ego.was.false <- clusters.with.disagreement[, list(ClusterID, average.without.ego = n.true / (total.n - 1)), ]
    ego.was.true <- clusters.with.disagreement[, list(ClusterID, average.without.ego = (n.true - 1) / (total.n - 1)), ]
    my.all.true <- my.dt.2[these.have.only.true.cases, list(ClusterID, average.without.ego = 1), ]
    my.all.false <- my.dt.2[these.have.zero.true.cases, list(ClusterID, average.without.ego = 0), ]
    
    ClusterID.data.ego.was.true.disagree <- my.dt[which(ClusterID %in% clusters.with.disagreement$ClusterID & source.var), list(ClusterID, PersonID.unique, source.var), ]
    ClusterID.data.ego.was.false.disagree <- my.dt[which(ClusterID %in% clusters.with.disagreement$ClusterID & ! source.var), list(ClusterID, PersonID.unique, source.var), ]
    
    ## prepare merging
    setkey(ClusterID.data.ego.was.true.disagree, ClusterID)
    setkey(ClusterID.data.ego.was.false.disagree, ClusterID)
    
    my.true.disagreement <- merge(ClusterID.data.ego.was.true.disagree, ego.was.true)
    my.false.disagreement <- merge(ClusterID.data.ego.was.false.disagree, ego.was.false)
    
    ## agreement
    ClusterID.data.ego.was.true.agree <- my.dt[which(! ClusterID %in% clusters.with.disagreement$ClusterID & source.var), list(ClusterID, PersonID.unique, source.var), ]
    ClusterID.data.ego.was.false.agree <- my.dt[which(! ClusterID %in% clusters.with.disagreement$ClusterID & ! source.var), list(ClusterID, PersonID.unique, source.var), ]
    
    ## prepare merging
    setkey(ClusterID.data.ego.was.true.agree, ClusterID)
    setkey(ClusterID.data.ego.was.false.agree, ClusterID)
    
    my.true.agreement <- merge(ClusterID.data.ego.was.true.agree, my.all.true)
    my.false.agreement <- merge(ClusterID.data.ego.was.false.agree, my.all.false)
    
    temp.1 <- rbind(my.true.agreement, my.false.agreement, my.true.disagreement, my.false.disagreement)
    temp.2 <- temp.1[, list(PersonID.unique, average.without.ego)]
    colnames(temp.2)[2] <- "average.without.ego"
    setkey(temp.2, PersonID.unique)
    return(temp.2)
}

justifyIPV_cluster <- contextuals.f("woman.thinks.IPV.can.be.justified", my.dt)
viol01_cluster <- contextuals.f("woman.exposed.to.IPV", my.dt)
setkey(my.dt, PersonID.unique)

for(x in c("justifyIPV_cluster", "viol01_cluster")){
    my.cluster.var <- get(x)
    colnames(my.cluster.var)[2] <- x
    my.dt <- merge(my.dt, my.cluster.var, all.x = TRUE, all.y = FALSE)
}

## match partners in a household by using matching V034 for women with PersonID.unique for men.
## empowerment.V034 contains the line number of the husband
## my.map records, for women with a husband, the index number of the man's record
## subset with that index number to add a feature of the husband onto the record of the wife.
my.map <- my.dt[, match(paste(HouseholdID, empowerment.V034, sep = "."), PersonID.unique), ]
my.dt[, husband.thinks.IPV.can.be.justified := man.thinks.IPV.can.be.justified[my.map], ]
my.dt[which(woman.thinks.IPV.can.be.justified &
            husband.thinks.IPV.can.be.justified),
      beat01_couple := "Both justify IPV", ]
my.dt[which(! woman.thinks.IPV.can.be.justified &
            ! husband.thinks.IPV.can.be.justified),
      beat01_couple := "No one justifys IPV", ]
my.dt[which(! woman.thinks.IPV.can.be.justified &
            husband.thinks.IPV.can.be.justified),
      beat01_couple := "Husband justifys IPV", ]
my.dt[which(woman.thinks.IPV.can.be.justified &
            ! husband.thinks.IPV.can.be.justified),
      beat01_couple := "Wife justifys IPV", ]

## How are decisions on made?
decisions.f <- function(answer.from.wife, answer.from.husband){
    results <- rep(NA, length(answer.from.wife))
    results[which(answer.from.wife == "Respondent and husband/partner" &
                  answer.from.husband == "Respondent and wife/partner")] <- "joint decisions"
    results[which(answer.from.wife == "Respondent alone" &
                  answer.from.husband == "Wife/partner alone")] <- "wife decides"
    results[which(answer.from.wife == "Husband/partner alone" &
                  answer.from.husband == "Respondent alone")] <- "husband decides"
    ## Wife says I decide, husband says something else
    results[which(answer.from.wife == "Respondent alone" &
                  answer.from.husband != "Wife/partner alone")] <- "don't agree"
    ## Wife says my husband decides, husband says something else
    results[which(answer.from.wife == "Husband/partner alone" &
                  answer.from.husband != "Respondent alone")] <- "don't agree"
    ## Wife says we decide together, partners says something else
    results[which(answer.from.wife == "Respondent and husband/partner" &
                  answer.from.husband != "Respondent and wife/partner")] <- "don't agree"
    ## Wife says Someone else decide, partners says something else
    results[which(answer.from.wife == "Someone else" &
                  answer.from.husband != "Someone else")] <- "don't agree"
    ## Wife OR husband says "Respondent and other person"
    results[which(answer.from.wife == "Respondent and other person" |
                  answer.from.husband == "Respondent and other person")] <- "don't agree"
    return(results)
}

## How are decisions made on health care and large purchases
my.dt[, decide2_health := decisions.f(empowerment.V743A, empowerment.MV743A[my.map]), ]
my.dt[, decide2_purchases := decisions.f(empowerment.V743B, empowerment.MV743B[my.map]), ]

## Educational differences: edudeviation
## Make V149 and MV149 ordered factors with this ordering
education.levels.ordered <- c("No education", "Incomplete primary", "Complete primary", "Incomplete secondary", "Complete secondary", "Higher")
my.dt[ , empowerment.V149 := factor(empowerment.V149, levels = education.levels.ordered, ordered = TRUE), ]
my.dt[ , empowerment.MV149 := factor(empowerment.MV149, levels = education.levels.ordered, ordered = TRUE), ]
my.dt[which(empowerment.V149 == empowerment.MV149[my.map]), edudeviation := "No difference", ]
my.dt[which(as.numeric(empowerment.V149) < as.numeric(empowerment.MV149[my.map])), edudeviation := "Husband has higher edu.", ]
my.dt[which(as.numeric(empowerment.V149) > as.numeric(empowerment.MV149[my.map])), edudeviation := "Wife has higher edu.", ]

## ln_numb_of_children, 
## Björns original used BORDX which includes dead children.
my.dt[ , ln_numb_of_children := log(0.5 + apply(cbind(empowerment.V202, empowerment.V203, empowerment.V204, empowerment.V205), 1, sum)), ]

my.dt[ , lnage_wife := log(age), ]

## For all data points that we have coordinates for, we use
## point-in-polygon techniques to match Subnational GDI and
## Subnational GNI from the Global Data Lab.

## Afghanistan lacks coordinate data, but the region names used by DHS
## corresponds the 34 gadm areas at level 1, so with good enough
## precision we can locate each case. For Afghanistan the polygons
## provided by the GDL are coarser so precision is not lost.

## Step 1, match RegionID from DHS with names of GADM region level 1
Afghanistan.Region.ID.from.dhs <- car::recode(my.dt[country.code.ISO.3166.alpha.3 == 4, substring(RegionID, 4), ], recodes = "'Helmand' = 'Hilmand'; 'Herat' = 'Hirat'; 'Kunarha' = 'Kunar'; 'Nooristan' = 'Nuristan'; 'Panjsher' = 'Panjshir'; 'Sar-e-pul' = 'SariPul'; 'Urozgan' = 'Uruzgan'")

## Update the RegionID in my.dt for these cases.
my.dt[country.code.ISO.3166.alpha.3 == 4, RegionID := paste0("AF.", Afghanistan.Region.ID.from.dhs), ]

## For each unique value of RegionID, count the number of clusters
n.per.region <- my.dt[country.code.ISO.3166.alpha.3 == 4, length(unique(ClusterID)), by = RegionID]
clusters.per.region <- my.dt[country.code.ISO.3166.alpha.3 == 4, unique(ClusterID), by = RegionID]

## For now, just sample coordinates in the correct GADM region.
## sampling points using the spatstat.random::rpoint(), which requires projected polygons

## Get polygons for level 1 in Afghanistan
## Define the URL and destination file
if(! file.exists("gadm41_AFG_1.json.zip")){
    url <- "https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_AFG_1.json.zip"
    destfile <- "gadm41_AFG_1.json.zip"

    ## Download the file
    utils::download.file(url, destfile, mode = "wb")

    ## Unzip the file
    utils::unzip(destfile)
}

## read the shapes
afghanistan <- sf::st_read(paste("gadm41_AFG_1.json"))

## project the shapes
afghanistan.level1.projected <- sf::st_transform(afghanistan[, "NAME_1"], "EPSG:3857")

sample.points.f <- function(gadm.region.name, n, polygons){
    my.win <- spatstat.geom::as.owin(polygons[match(gadm.region.name, polygons$NAME_1), ])
    spatstat.random::rpoint(n = n, win = my.win)
}

## Generate coordinates for all cases from Afghanistan based on their RegionID by sampling.
afghanistan.coordinates = data.table::rbindlist(sapply(unique(Afghanistan.Region.ID.from.dhs), function(region.name) {
    my.n <- n.per.region$V1[match(paste0("AF.", region.name), n.per.region$RegionID)]
    points_sampled <- sample.points.f(gadm.region.name = region.name, n = my.n, polygons = afghanistan.level1.projected)
    ## Convert to sf
    sf_points <- sf::st_as_sf(data.frame(x = points_sampled$x, y = points_sampled$y), 
                          coords = c("x", "y"), crs = 3857)

    ## Transform to lon/lat (EPSG 4326)
    sf_points_lonlat <- sf::st_transform(sf_points, crs = 4326)

    ## Extract coordinates
    coords <- sf::st_coordinates(sf_points_lonlat)

    ## Find the unique ClusterID:s in this region.
    ClusterID = as.character(clusters.per.region$V1[clusters.per.region$RegionID %in% paste0("AF.", region.name)])
    
    return(data.table::data.table(ClusterID, lon = coords[, "X"], lat=coords[, "Y"]))
}, simplify = FALSE))

## impute longitude and latitude for the cases from Afghanistan

for(i in 1:nrow(afghanistan.coordinates)){
    these <- which(my.dt$ClusterID == afghanistan.coordinates$ClusterID[i])
    my.dt[these, lon := rep(afghanistan.coordinates$lon[i], length(these)), ]
    my.dt[these, lat := rep(afghanistan.coordinates$lat[i], length(these)), ]    
}

gdi <- setDT(read.csv(paste0(gdl.path, "GDL-Subnational-GDI-data.csv"), check.names = FALSE))
## keep only the necessary columns from hdi and gdi
## GDI has no data in columns "older" than 2000
my.cols <- c("GDLCODE", 2000:2022)
gdi.clean <- gdi[ , my.cols, with = FALSE]

## use st_intersects() to map points to polygons
## apply st_intersects() only once per unique coordinate
## create the set of all unique points
unique.pts <- unique(na.omit(my.dt[, list(lon, lat), ]))
sf_points <- sf::st_as_sf(unique.pts, coords = c("lon", "lat"), crs = 4326)

## There are four invalid polygons in gdl.polygons, correct them.
## invalid_polygons <- which(!st_is_valid(gdl.polygons))
## invalid_polygons
## [1]  105  544 1414 1667
gdl.polygons <- sf::st_make_valid(sf::st_read(paste0(gdl.path, "GDL Shapefiles V6.4")))
gdl.classification <- sf::st_intersects(sf_points, gdl.polygons)
## Get the area names from the result of st_intersects
indices <- sapply(gdl.classification, function(x) { x[1] })
gdl.codes <- gdl.polygons$gdlcode[indices]
unique.pts$GDLCODE <- gdi.clean$GDLCODE[match(gdl.codes, gdi.clean$GDLCODE)]
unique.pts[, lon.lat.identifier := paste(lon, lat, sep = "."), ]
unique.pts.clean <- unique.pts[ , list(lon.lat.identifier, GDLCODE), ]

## convert gdi.clean from wide to long
# Convert to long format
gdi.long <- melt(gdi.clean, id.vars = "GDLCODE", 
                variable.name = "year.of.interview", 
                value.name = "GDI")
gdi.long[, year.of.interview := as.numeric(as.character(year.of.interview)), ]

## Now that we know which GLDCODE for which coordinate, create a data set with ClusterID, coordinates, and year, and add the GDL data to that set.

my.temp.set <- unique(my.dt[which(is.na(lon) == FALSE), list(ClusterID, lon.lat.identifier = paste(lon, lat, sep = "."), year.of.interview), ])
## add GDLCODE to my.temp.set, using unique.pts.clean
setkey(my.temp.set, lon.lat.identifier)
setkey(unique.pts.clean, lon.lat.identifier)
my.temp.set.2 <- merge(my.temp.set, unique.pts.clean)
## The lon.lat.identifier is no longer useful
my.temp.set.2[, lon.lat.identifier := NULL, ]

setkey(my.temp.set.2, GDLCODE, year.of.interview)
setkey(gdi.long, GDLCODE, year.of.interview)
my.temp.set.3 <- merge(my.temp.set.2, gdi.long)

## Repeat with GNI
## Gross National Income per Capita in 1000 US-Dollars (2011 PPP)
gni <- setDT(read.csv(paste0(gdl.path, "GDL-Log-Gross-National-Income-per-capita-in-1000-US-Dollars-(2011-PPP)-data.csv"), check.names = FALSE))
gni.clean <- gni[ , my.cols, with = FALSE]
gni.long <- melt(gni.clean, id.vars = "GDLCODE", 
                variable.name = "year.of.interview", 
                value.name = "GNI")
gni.long[, year.of.interview := as.numeric(as.character(year.of.interview)), ]
setkey(gni.long, GDLCODE, year.of.interview)
my.temp.set.4 <- merge(my.temp.set.3, gni.long)

## If GDLCODE is no longer useful, drop it
my.temp.set.4[, GDLCODE := NULL, ]

## merge back to my.dt
setkey(my.temp.set.4, ClusterID, year.of.interview)
setkey(my.dt, ClusterID, year.of.interview)
my.dt.temp <- merge(my.dt, my.temp.set.4)

## rename columns to match previous data file
colnames(my.dt.temp) <- gsub("woman.exposed.to.IPV", "viol01_wife", colnames(my.dt.temp))
colnames(my.dt.temp) <- gsub("iwi", "iwi2", colnames(my.dt.temp))
colnames(my.dt.temp) <- gsub("ClusterID", "cluster1", colnames(my.dt.temp))
colnames(my.dt.temp) <- gsub("GDI", "gdi_s", colnames(my.dt.temp))
colnames(my.dt.temp) <- gsub("GNI", "gni_s", colnames(my.dt.temp))
colnames(my.dt.temp) <- gsub("empowerment.V149", "v149", colnames(my.dt.temp))

## Convert the country code from numeric to letters
my.dt.temp[, country := iso.3166$String.code[match(country.code.ISO.3166.alpha.3, iso.3166$numeric)], ]

## ## Remove data that appears to be artifacts rather than real data
## ## viol01_wife
## MMR                         0.6000000 0.40000000     6
## SWZ                               NaN        NaN     1
## TCD                         1.0000000 0.00000000     3

## What is wrong with these? Honduras, Moldova and Timor-Leste
## HND                         1.0000000 0.00000000  3304
## MDA                         1.0000000 0.00000000   939
## TLS                         1.0000000 0.00000000  3781

## SLE Sierra Leone is also weird, but not necessarily wrong.
## SLE                         0.5000000 0.50000000  8594

my.dt.temp <- my.dt.temp[which(country %in% c("HND", "MDA", "MMR", "SWZ", "TCD", "TLS") == FALSE), ]

## Use the string instead of the digit code, so we get more informative labels
my.dt.temp[, country := iso.3166$String[match(country.code.ISO.3166.alpha.3, iso.3166$numeric)], ]

## keep only the needed columns, and only complete cases, this set will
## only be used for the regression which requires non-missing data.

my.dt.clean <- my.dt.temp[which(is.na(beat01_couple) == FALSE &
                                is.na(viol01_wife) == FALSE &
                                is.na(v149) == FALSE &
                                is.na(decide2_purchases) == FALSE &
                                is.na(decide2_health) == FALSE & 
                                is.na(ln_numb_of_children) == FALSE &
                                is.na(lnage_wife) == FALSE &
                                is.na(iwi2) == FALSE),                                
                          list(viol01_wife, beat01_couple, v149, edudeviation, decide2_purchases, decide2_health, gdi_s, gni_s, viol01_cluster, justifyIPV_cluster, iwi2, country, cluster1, ln_numb_of_children, lnage_wife, lon, lat), ]

## z-standardize some numeric variables
for(var in c("gni_s", "iwi2")){
    my.dt.clean[[var]] <- scale(my.dt.clean[[var]])
}

## Treat the states of India as countries (note that they already have separate values for GDI and GNI, the country change only affect the random intercept).
these <- which(my.dt.clean$country == "India")
my.dt.clean$country[these] <- as.vector(sapply(as.character(my.dt.clean$cluster1[these]), function(x) {
   strsplit(x, ".", fixed = TRUE)[[1]][2]
}))

write.csv(my.dt.clean, file = "ipv-women2.csv", row.names = FALSE)
save(my.dt.clean, file = "my.dt.clean.RData")

## Also save a copy of the full set (including all vars, and cases
## with missing data)
my.dt <- my.dt.temp
save(my.dt, file = "full.set.RData")

## Create the two sets needed for kriging: these need to have
## non-missing on lon, lat, but justifyIPV_cluster does not require
## the cases to have non-missing on viol01_cluster. Since
## justifyIPV_cluster is asked in more surveys, use that extra data!

justifyIPV_cluster <- my.dt[which(! is.na(lon) & ! is.na(justifyIPV_cluster)), list(country.code.ISO.3166.alpha.3, lon, lat, justifyIPV_cluster), ]
viol01_cluster <- my.dt[which(! is.na(lon) & ! is.na(viol01_cluster)), list(country.code.ISO.3166.alpha.3, lon, lat, viol01_cluster), ]

## clean up
rm(my.dt, my.dt.temp, my.temp.set.4, my.temp.set.3, my.temp.set.2, my.temp.set, gdi, gni, gdi.clean, gni.clean, gdi.long, gni.long, my.cols, my.map, unique.pts.clean, unique.pts, indices, gdl.polygons, gdl.classification, gdl.codes, sf_points, education.levels.ordered)
gc(verbose = FALSE)

@

\subsection{Adding data for observations without coordinates}
\label{sec:adding-data-observ}
There are countries, e.g. Afghanistan, which have relevant data, but lacks geographical coordinates. To merge these data with external data (the subnational Gender Equity Index GDI and the subnational productivity statistic GNI) used as control variables in the regression requires another strategy.


\section{Mapping IPV - Exposure and Attitudes }

Apply \href{Kriging}{https://en.wikipedia.org/wiki/Kriging} on the data. Kriging gives a predicted smooth surface for the estimate of the dependent variable in an area. While Kriging can use (multiple) independent variables in this estimation, here it is used without any independent variables, so the input data used is only the coordinates the observations, and the observed value on the dependent variable at those coordinates. The procedure is applied on two different outcomes, the proportion of women in local area who thinks IPV can be justified, and the proportion of women in the local area that have experienced IPV by their current husband. Technically, Kriging predicts values for a large number of points in a grid covering the area of interest, and the resolution of the grid, the width and height of each cell, was set to 10 kilometers, i.e. 100 square kilometers. Disconnected areas, e.g. islands, were estimated separately while all adjacent countries were estimated together. For South east Asia, the total number of cells for which we estimated a value is 64287; Africa and Madagascar covered [fill in number here].

The coordinate data is originally in longitude and latitude, i.e. referencing locations on a sphere. The Kriging algorithm assumes a flat coordinate system, so we project the coordinate data to the webmercator refence system (the code for the webmercator system is «EPSG:3857«)

In rare cases a single local area can be surveyd twice, or the random replacement algorithm DHS uses to enhance the integrity of the respondends can make two different areas end up having the sam coordinate, but the the Kriging algorithm in use here assumes a single unique values per coordinate. To solve this, we calculate the mean value for all unique coordinates.

We have made two versions of these maps: one version uses only the latest measurements of IPV and attitudes toward IPV, the over version uses all available data which means that in a single country there can be data from multiple survey waves, i.e. repeated cross-sectional data. The mapping algorithm is only spatial, not spatio-temporal, which means the predictions are not for any specified point in time and the all measurements are weighted equally regardless of when they are made.

The bounding polygon for the Kriging process is created by merging the borders of all adjacent countries (in case of islands and or other disconnected areas, the country border is used directly). When preprocessing of the data we use the «R« packages «sp« and «raster«. The Kriging is done by \href{https://cran.r-project.org/web/packages/autoFRK/index.html}{«autoFRK«}, a package that implements a special version of Kriging that can work with massive amounts of datapoints \cite{Tzeng03042018}. The analyses are reproducible from scratch by the scripts we provide in the source code for this document.

\subsection{Prepare input data for Kriging}

The respondents live in disconnected parts of the world, and we can assume that disconnected areas can be processed separately, ie that we do not lose valueable information by running separate analyses for Africa and South east Asia, or even by running a separate analysis on islands as Madagascar and the Philippines, or even other disconnected areas. Some countries are disconnected because there is no data for their neighbouring countries, e.g. Egypt and Cambodia.

Construct a set of tables, one for each disconnected area, with the following structure: coordinates, dependent variable (either the proportion exposed to IPV, or the proportion women who thinks IPV can be justified).

\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \toprule
 & lon & lat & viol01\_cluster \\ 
  \midrule
1 & 29.570 & -2.733 & 0.000 \\ 
  2 & 25.835 & -17.862 & 0.231 \\ 
  3 & 9.691 & 12.436 & 0.032 \\ 
  4 & 79.477 & 25.206 & 0.111 \\ 
  5 & 86.884 & 25.758 & 0.714 \\ 
  6 & 79.437 & 17.020 & 0.333 \\ 
   \bottomrule
\end{tabular}
\end{table}

The coordinates will be converted from longitude, latitude (which uses degree as units) into the webmercator reference system (which uses meter as the unit) before kriging is applied.

\subsection{Define the "islands"}
\label{sec:define-islands}

It turns out that due to data availablility Africa is divided into two disconnected large areas: west and (south-)east.

The disconnected parts in Africa.

<<define.islands.africa, tidy = FALSE>>=
Madagascar <- "Madagascar"
Egypt <- "Egypt"
West <- c("Benin", "Burkina Faso", "Cameroon", "Chad", "Côte d'Ivoire", "Gabon",
          "Ghana", "Guinea", "Liberia", "Mali", "Mauritania", "Nigeria",
          "Senegal", "Gambia", "Sierra Leone", "Togo")
South.and.east <- c("Zambia", "Zimbabwe", "South Africa",
     "United Republic of Tanzania", "Uganda", "Rwanda", "Namibia", "Mozambique",
     "Malawi", "Lesotho", "Kenya", "Ethiopia",
     "Democratic Republic of the Congo", "Burundi", "Angola")
@

The disconnected parts in South Asia.

<<define.islands.asia>>=
india.et.al <- c("Myanmar", "India", "Bangladesh", "Nepal", "Pakistan", "Afghanistan")
philippines <- "Philippines"
timor <- "Timor-Leste"
cambodia <- "Cambodia"
@

\subsection{Prepare data for each continent}
\label{sec:define-function-}

The function «generate.input.data.for.kriging.f()« defined below creates a set of tables, and the relevant country borders, for a given set of countries.

\begin{compactenum}
\item Select the relevant subset of observations for this set of countries.
\item Discard observations with either missing coordinates or missing data.
\item Take the mean of the dependent variable for each unique coordinate pair.
\item Create a geographical object based in the coordinates.
\item Project the geographical object to the webmercator coordinate system.
\item Export the projected coordinates and the observations to an ordinary table.
\item Import the borders for each country in the set.
\item If there are multiple countries in the set, merge the country borders to a single polygon.
\item Project the polygon from lon/lat to webmercator.
\item Create a grid of points, separated by 10.000 meters, based on the extent of the polygon.
\item Set all points in the grid that is outside the polygon to NA, to indicate that they are not relevant.
\item Save the coordinates of all points that are not NA to an ordinary table.
\end{compactenum}

This creates the two tables used for kriging (the observations), and predictions (the gridpoints) both in the correct reference system. In addition, also create the country borders that should be displayed on the maps. The country border data comes from the «R« package «rnaturalearthdata« accessed with the package «rnaturalearth«. To make the final maps less cluttered, the function accepts an argument for which country borders to ignore.

<<set.gis.path>>=
gis.map <- paste0(my.dir, "/GIS-borders/")
@

<<generate.input.data.for.kriging>>=
generate.input.data.for.kriging.f <- function(my.set, my.var, my.clean.dt, projection = "EPSG:3857", my.cell.width = 10000, country.borders.to.ignore = NULL){
    ## country (as string) is used to match against country.borders.to.ignore
    my.clean.dt[, country := iso.3166$String[match(country.code.ISO.3166.alpha.3, iso.3166$numeric)], ]

    ## Select the relevant data points 
    my.dt.with.data <- my.clean.dt[ 
        country.code.ISO.3166.alpha.3 %in% iso.3166$numeric[match(my.set, iso.3166$String)] &
        ! is.na(lon) &
        ! is.na(my.var), ]

    if(nrow(my.dt.with.data) == 0){
        return(NULL)
    }

    ## only save one observation per coordinate pair,
    ## take the mean value of my.var at each unique coordinate
    my.dt.with.data[, coordinate := paste(lon, lat, sep = "."), ]
    my.dt <- my.dt.with.data[, list(temp = mean(get(my.var), na.rm = TRUE), lon=unique(lon), lat = unique(lat)), by = coordinate]
    my.dt[, coordinate := NULL, ]

    ## Create simple feature collection of points
    foo <- sf::st_as_sf(my.dt, coords = c("lon", "lat"), crs = 4326)

    ## Project from long/lat to webmercator
    points.with.data <- sf::st_transform(foo, projection)

    ## First column is the observed value, second column is `geometry`.
    ## use sf::st_coordinates(points.with.data) to get coordinates in a way
    ## that autoFRK can use.
    ## autoFRK(Data = points.with.data[[1]], loc = sf::st_coordinates(points.with.data))

    ## import the borders for these countries
    world <- rnaturalearth::ne_countries(scale = 50, returnclass = "sv")
    borders <- world[as.numeric(world$iso_n3_eh) %in% unique(my.dt.with.data$country.code.ISO.3166.alpha.3), ]
    projected.countries <- terra::project(borders, "EPSG:3857")

    ## Create grid points with terra
    template <- terra::rast(projected.countries, resolution = c(my.cell.width, my.cell.width))
    terra::values(template) <- rep(1, terra::ncell(template))
    r <- terra::mask(template, projected.countries)    
    # Get indices of the cells contained in the polygon
    valid.cells <- which(!is.na(terra::values(r)))
    ## Extract midpoints of all those cells
    my.grid.points <- as.data.frame(terra::xyFromCell(r, valid.cells))

    ## If requested, create a subset of borders to actually plot (use the inverse)
    all.countries <- unique(my.dt.with.data$country)
    if(is.null(country.borders.to.ignore) == FALSE & length(which(country.borders.to.ignore %in% all.countries)) > 0){
        names.of.countries.to.include <- all.countries[which(all.countries %in% country.borders.to.ignore == FALSE)]
        if(length(names.of.countries.to.include) > 0){
            borders.to.plot <- world[world$admin %in% names.of.countries.to.include, ]
        } else {
            borders.to.plot <- NULL
        }
    } else {
        borders.to.plot <- borders
    }
    
    ## The first two elements are for kriging, the last two elements are useful for
    ## plotting the end results: [3] has all borders, useful to crop the end raster
    ## [4] has the borders to include in the plot.
    
    ## Note that the last obect is a SpatVector object, so make sure terra is loaded
    ## And terra version 1.7.0 is required for wrap()

    ## Allow NULL values
    if(is.null(borders) == FALSE){
        borders <- terra::wrap(borders)
    }
    if(is.null(borders.to.plot) == FALSE){
        borders.to.plot <- terra::wrap(borders.to.plot)
    }
    return(list(points.with.data, my.grid.points, borders, borders.to.plot))
}

@

Configure the system for parallel computing.

<<start.future, cache = FALSE>>=
library(doFuture); library(globallivingconditions); library(sf)
options(future.gc=TRUE, mc.cores = parallelly::availableCores(method = c("Slurm", "system")))
future::plan(future::multicore)
doFuture::registerDoFuture()

@

A function that process all adjacent countries simultaneously, and aligns and merges the results to a single raster. The slighly higher resolution used here is to avoid artifacts (undefined vertical stripes) when the different raster are aligned to a single origin.

The «regularisation.factor« argument provides a way for the user to choose how strong regularisation to use. The default in «autoFRK()« is to use $ 10 \sqrt{n} $. A higher value gives the kriging algorithm more freedom to care for local deviances, resulting in a more detailed map, while a lower value forces the kriging to make a smoother surface, or, in other words, to keep only the most important trends in the data. Too high values risk overfitting, while too low values gives a too simplified result. Given that the number of respondents per site typically is rather low, we have choosen to use a stronger regularisation than the default of «autoFRK()« - we use $ \sqrt{n} $ as «maxK«, the maximum number of basis functions considered.


<<kriging.full.pipeline>>=
kriging.full.pipeline.f <- function(prepared.data, resolution = my.resolution * 1.1, regularisation.factor = 10){
    ## Africa or Asia, each with its own set of islands
    library(data.table); library(sf)

    my.raster <- lapply(1:length(prepared.data), function(j) {
        my.fit <- autoFRK::autoFRK(Data = prepared.data[[j]][[1]][[1]], loc = sf::st_coordinates(prepared.data[[j]][[1]]), 
                                                maxK = round(regularisation.factor * sqrt(length(prepared.data[[j]][[1]][[1]]))))
        my.predictions <- predict(my.fit, newloc = prepared.data[[j]][[2]])
        kriging.results <- my.predictions$pred.value[, 1]

        ## Convert new locations to an sf object points collection
        my.sf <- sf::st_as_sf(
            data.frame(prepared.data[[j]][[2]], viol01_cluster = kriging.results), 
            coords = c("x", "y"), crs = 3857)
    
        ## Create an empty raster with terra
        r <- rast(ext(my.sf), resolution = resolution, crs = "EPSG:3857")
    
        ## Rasterize
        raster.mean <- rasterize(my.sf, r, field = "viol01_cluster", fun = mean)
    })    

    ## Compute the union of extents
    common.extent <- Reduce(terra::union, lapply(my.raster, terra::ext))

    # Create a template raster with the common extent and the resolution of the first raster
    template <- terra::rast(ext=common.extent, res=terra::res(my.raster[[1]]), crs=terra::crs(my.raster[[1]]))
    
    ## Extend all rasters to this template
    aligned.rasters <- lapply(my.raster, function(r) terra::resample(r, template, method="bilinear"))

    ## Stitch the parts together
    complete.raster <- do.call(terra::mosaic, aligned.rasters)

    ## Transform back to longitude latitude
    raster.long.lat <- terra::project(complete.raster, "EPSG:4326")

    ## borders.to.plot is more difficult, since it can contain empty elements
    valid.vectors <- sapply(1:length(prepared.data), function(j) {
        if(is.null(prepared.data[[j]][[4]])){
            return(NULL)
        }
        sv <- terra::unwrap(prepared.data[[j]][[4]])
        if (geomtype(sv) == "polygons") return(sv) else return(NULL)
    }, simplify = FALSE)

    ## Remove NULLs
    valid.vectors <- Filter(Negate(is.null), valid.vectors)

    ## Ensure there's something left to union
    if (length(valid.vectors) > 0) {
        borders.to.plot <- Reduce(terra::union, valid.vectors)
    } else {
        borders.to.plot <- NULL  # Or handle the empty case as needed
    }
    return(list(raster.long.lat, borders.to.plot))
}

@

Create data suitable for kriging exposure to IPV in Asia.

<<exposure-asia-prepare>>=
my.resolution = 10000

remove.empty.elements.f <- function(lst) {
  lst[!sapply(lst, function(x) length(x) == 0)]
}

asia.exposure.input.data <- remove.empty.elements.f(foreach::foreach(set = list(india.et.al, philippines, cambodia),
        .options.future = list(scheduling = TRUE,
                               seed = TRUE,
                               packages = c("data.table", "globallivingconditions"))
        ) %dofuture% { print(paste(set)); generate.input.data.for.kriging.f(my.set = set, my.var = "viol01_cluster", my.clean.dt = viol01_cluster, my.cell.width = my.resolution, country.borders.to.ignore = c("India", "Cambodia", "Philippines")) })
@

Calculate IPV exposure in Asia. Serialise to make the results cacheable. (Raster objects consist of pointers to objects stored on disk, these pointers are locally defined and become invalid when the function returns. To avoid this, the function «wrap()« makes a full copy of the raster object to RAM. When these objects are later used, «unwrap()« has to be applied to make them ordinary raster objects again. Together «wrap()« and «unwrap()« are essential whenever a function is returning raster objects or when working with «knitr« which caches the results of chunks to avoid having to redo the computations every time the source is compiled to a \LaTeX document).

<<asia-IPV-exposure-calc, dependson="kriging.full.pipeline">>=
results <- kriging.full.pipeline.f(asia.exposure.input.data, regularisation.factor = 1)
if(is.null(results[[1]]) == FALSE){
    serializable.raster <- terra::wrap(results[[1]])
} else {
    serializable.raster <- NULL
}
if(is.null(results[[2]]) == FALSE){
    serializable.border <- terra::wrap(results[[2]])
} else {
    serializable.border <- NULL
}
@

Calculate sample size for Exposure in Asia

<<n.stats, tidy = FALSE>>=
countries.codes.asia <- c("Andaman And Nicobar Islands",
   "Andhra Pradesh", "Arunachal Pradesh", "Assam",
   "Bihar", "Chandigarh", "Chhattisgarh", "Goa", "Gujarat",
   "Haryana", "Himachal Pradesh", "Jammu And Kashmir",
   "Jharkhand", "Karnataka", "Kerala", "Lakshadweep",
   "Madhya Pradesh", "Maharashtra", "Manipur", "Meghalaya",
   "Mizoram", "Nagaland", "New Delhi", "Odisha", "Puducherry",
   "Punjab", "Rajasthan", "Sikkim", "Tamil Nadu",
   "Telangana", "Tripura", "Uttar Pradesh", "Uttarakhand",
   "West Bengal", "KHM", "PAK", "MMR", "NPL", "PHL", "BGD")
n.persons.exposure.asia <- my.dt.clean[country %in%
  countries.codes.asia, length(which(is.na(viol01_wife)
  == FALSE)), ]
n.local.areas.exposure.asia <- my.dt.clean[country %in%
  countries.codes.asia &
  is.na(viol01_wife) == FALSE, length(unique(cluster1)), ]
n.surveys.exposure.asia <- length(unique(sapply(strsplit(as.character(
    my.dt.clean$cluster1), ".", fixed = TRUE), function(x) {
        paste(x[c(1, 3)], collapse = ".") })))

@ 

Plot exposure to IPV in Asia.

<<asia-IPV-exposure, fig.cap = paste0("Estimated proportion of women in the local area who have been exposed to violence by their current husband, Asia. Pakistan, India, Nepal, Myanmar, Cambodia and Philippines. Country borders in darkgreen, and countries without data in gray. Number of women = ", n.persons.exposure.asia, ", number of local areas = ", n.local.areas.exposure.asia, ". Kriging performed on data compiled from ", n.surveys.exposure.asia, " surveys funded by the DHS."), dependson = "asia-IPV-exposure-calc", fig.width = 9, fig.height = 5, fig.align = "center", tidy = FALSE>>=
my.palette.f <- grDevices::colorRampPalette(c("blue", "white", "red"))
my.raster <- terra::clamp(unwrap(serializable.raster),
                          lower=0, upper=1, values=TRUE)
terra::plot(my.raster, col = my.palette.f(101), range = c(0, 1),
            plg = list(at = seq(from = 0, to = 1, by = 0.1)),
            axes = TRUE, ylim = c(4, 38), xlim = c(60, 127))
## terra::contour(my.raster, add = TRUE, levels = c(0.25, 0.5, 0.75))
if(is.null(serializable.border) == FALSE){
    my.borders <- unwrap(serializable.border)
    plot(my.borders, add = TRUE, col = NA, border = "darkgreen", lwd = 2)
}

## add adjacent countries with no data in gray
## world$admin has names, but not proper names according to iso.3166
## eg "Laos" instead of "Lao People's Democratic Republic"
## Therefore, use the numeric code in world$iso_n3_eh instead
add.gray.countries <- c("Bangladesh", "Viet Nam",
                        "Lao People's Democratic Republic", "Thailand")
world <- rnaturalearth::ne_countries(scale = 50, returnclass = "sv")
gray.countries <- world[as.numeric(world$iso_n3_eh) %in%
        iso.3166$numeric[match(add.gray.countries, iso.3166$String)], ]
plot(gray.countries, col = "gray80", add = TRUE, border = "darkgreen", lwd = 0.5)

@

Repeat with attitudes to IPV.

<<n.stats.attitudes.asia>>=
n.persons.attitudes.asia <- my.dt.clean[country %in% countries.codes.asia, length(which(is.na(beat01_couple) == FALSE)), ]
n.local.areas.attitudes.asia <- my.dt.clean[country %in% countries.codes.asia &
  is.na(beat01_couple) == FALSE, length(unique(cluster1)), ]
n.surveys.attitudes.asia <- length(unique(sapply(strsplit(as.character(my.dt.clean$cluster1), ".", fixed = TRUE), function(x) { paste(x[c(1, 3)], collapse = ".") })))

@ 

<<asia-IPV-attitudes-calc>>=
asia.attitudes.input.data <- remove.empty.elements.f(foreach::foreach(set = list(india.et.al, philippines, cambodia),
        .options.future = list(scheduling = TRUE,
                               seed = TRUE,
                               packages = c("data.table", "globallivingconditions"))
        ) %dofuture% generate.input.data.for.kriging.f(my.set = set, my.var = "justifyIPV_cluster", my.clean.dt = justifyIPV_cluster, my.cell.width = my.resolution, country.borders.to.ignore = c("India", "Cambodia", "Philippines")))
results <- kriging.full.pipeline.f(asia.attitudes.input.data, regularisation.factor = 1)

if(is.null(results[[1]]) == FALSE){
    serializable.raster <- terra::wrap(results[[1]])
} else {
    serializable.raster <- NULL
}
if(is.null(results[[2]]) == FALSE){
    serializable.border <- terra::wrap(results[[2]])
} else {
    serializable.border <- NULL
}

@

<<asia-IPV-attitudes-plot, fig.cap = paste0("Estimated proportion of women in the local area who think IPV can be justified, Asia. Pakistan, India, Nepal, Myanmar, Cambodia and Philippines. Country borders in darkgreen, and countries without data in gray. Number of women = ", n.persons.attitudes.asia, ", number of local areas = ", n.local.areas.attitudes.asia, ". Kriging performed on data compiled from ", n.surveys.attitudes.asia, " surveys funded by the DHS."), fig.width = 9, fig.height = 5>>=
my.raster <- terra::clamp(terra::unwrap(serializable.raster), lower=0, upper=1, values=TRUE)
terra::plot(my.raster, col = my.palette.f(101), range = c(0, 1), plg = list(at = seq(from = 0, to = 1, by = 0.1)), axes = TRUE, ylim = c(4, 38), xlim = c(60, 127))
## terra::contour(my.raster, add = TRUE, levels = c(0.25, 0.5, 0.75))
if(is.null(serializable.border) == FALSE){
    my.borders <- unwrap(serializable.border)
    plot(my.borders, add = TRUE, col = NA, border = "darkgreen", lwd = 2)
}
add.gray.countries <- c("Bangladesh", "Viet Nam", "Lao People's Democratic Republic", "Thailand")
world <- rnaturalearth::ne_countries(scale = 50, returnclass = "sv")
gray.countries <- world[as.numeric(world$iso_n3_eh) %in% iso.3166$numeric[match(add.gray.countries, iso.3166$String)], ]
plot(gray.countries, col = "gray80", add = TRUE, border = "darkgreen", lwd = 0.5)

@ 

Repeat with African countries.

<<africa-IPV-attitudes-calc>>=
africa.attitudes.input.data <- remove.empty.elements.f(foreach::foreach(set = list(Madagascar, Egypt, South.and.east, West),
        .options.future = list(scheduling = TRUE,
                               seed = TRUE,
                               packages = c("data.table", "globallivingconditions"))
        ) %dofuture% generate.input.data.for.kriging.f(my.set = set, my.var = "justifyIPV_cluster", my.clean.dt = justifyIPV_cluster, my.cell.width = my.resolution, country.borders.to.ignore = c("Egypt", "Madagascar")))
results <- kriging.full.pipeline.f(africa.attitudes.input.data, regularisation.factor = 5)

if(is.null(results[[1]]) == FALSE){
    serializable.raster <- terra::wrap(results[[1]])
} else {
    serializable.raster <- NULL
}
if(is.null(results[[2]]) == FALSE){
    serializable.border <- terra::wrap(results[[2]])
} else {
    serializable.border <- NULL
}

@

Calculate sample sizes for the African continent.

<<sample.size.africa, tidy = FALSE>>=
countries.africa <- c(Madagascar, Egypt, West, South.and.east)
n.persons.attitudes.africa <- my.dt.clean[country %in% countries.africa &
                                        is.na(lon) == FALSE,
                               length(which(is.na(beat01_couple) == FALSE)), ]
n.local.areas.attitudes.africa <- my.dt.clean[country %in% countries.africa &
 is.na(lon) == FALSE & is.na(beat01_couple) == FALSE, length(unique(cluster1)), ]
n.surveys.attitudes.africa <- length(unique(sapply(strsplit(as.character(
    my.dt.clean[country %in% countries.africa & is.na(lon) == FALSE &
                is.na(beat01_couple) == FALSE, cluster1]),
    ".", fixed = TRUE), function(x) { paste(x[c(1, 3)], collapse = ".") })))
@ 

<<africa-IPV-attitudes-plot, fig.cap = paste0("Estimated proportion of women in the local area who think IPV can be justified, Africa. Country borders in darkgreen, and countries without data in gray. Number of women = ", n.persons.attitudes.africa, ", number of local areas = ", n.local.areas.attitudes.africa, ". Kriging performed on data compiled from ", n.surveys.attitudes.africa, " surveys funded by the DHS."), fig.width = 5, fig.height = 5, fig.align = "center">>=
my.raster <- terra::clamp(terra::unwrap(serializable.raster), lower=0, upper=1, values=TRUE)

## Load and unserialize the raster vector for the whole of Africa
load(paste0(africa.map.path, "africa-lowres.RData"))
africa <- terra::unwrap(africa)

terra::plot(my.raster, col = my.palette.f(101), range = c(0, 1), plg = list(at = seq(from = 0, to = 1, by = 0.1)), axes = TRUE,
            xlim = terra::ext(africa)[c(1, 2)], ylim = terra::ext(africa)[c(3, 4)])

## Fill with gray
terra::plot(africa, col = "gray80", add = TRUE)

## Overwrite with proper data where available
terra::plot(my.raster, col = my.palette.f(101), range = c(0, 1), add = TRUE)

## Add the relevant borders
if(is.null(serializable.border) == FALSE){
    my.borders <- terra::unwrap(serializable.border)
    terra::plot(my.borders, add = TRUE, col = NA, border = "darkgreen", lwd = 2)
}

@

Calculate sample sizes for exposure to IPV in Africa.

<<sample.size.exposure.africa, tidy = FALSE>>=
n.persons.exposure.africa <- my.dt.clean[country %in% countries.africa &
                                        is.na(lon) == FALSE,
                               length(which(is.na(viol01_wife) == FALSE)), ]
n.local.areas.exposure.africa <- my.dt.clean[country %in% countries.africa &
                                             is.na(lon) == FALSE &
                                             is.na(viol01_wife) == FALSE,
                                             length(unique(cluster1)), ]
n.surveys.exposure.africa <- length(unique(sapply(strsplit(as.character(
    my.dt.clean[country %in% countries.africa &
                is.na(viol01_wife) == FALSE &
                is.na(lon) == FALSE, cluster1]),
    ".", fixed = TRUE), function(x) { paste(x[c(1, 3)], collapse = ".") })))
@ 

Perform kriging on exposure to IPV in Africa.

<<africa-IPV-exposure-calc>>=
africa.exposure.input.data <- remove.empty.elements.f(foreach::foreach(set = list(Madagascar, Egypt, South.and.east, West),
        .options.future = list(scheduling = TRUE,
                               seed = TRUE,
                               packages = c("data.table", "globallivingconditions"))
        ) %dofuture% generate.input.data.for.kriging.f(my.set = set, my.var = "viol01_cluster", my.clean.dt = viol01_cluster, my.cell.width = my.resolution, country.borders.to.ignore = c("Egypt", "Madagascar")))
results <- kriging.full.pipeline.f(africa.exposure.input.data, regularisation.factor = 5)

if(is.null(results[[1]]) == FALSE){
    serializable.raster <- terra::wrap(results[[1]])
} else {
    serializable.raster <- NULL
}
if(is.null(results[[2]]) == FALSE){
    serializable.border <- terra::wrap(results[[2]])
} else {
    serializable.border <- NULL
}

@

Plot exposure to IPV in Africa.

<<africa-IPV-exposure-plot, fig.cap = paste0("Proportion women in the local area who have exposed to violence by their current husband, Africa. Country borders in darkgreen, and countries without data in gray. Number of women = ", n.persons.exposure.africa, ", number of local areas = ", n.local.areas.exposure.africa, ". Kriging performed on data compiled from ", n.surveys.exposure.africa, " surveys funded by the DHS."), fig.width = 5, fig.height = 5, fig.align = "center">>=
my.raster <- terra::clamp(terra::unwrap(serializable.raster), lower=0, upper=1, values=TRUE)

## Load and unserialize the raster vector for the whole of Africa
load(paste0(africa.map.path, "africa-lowres.RData"))
africa <- terra::unwrap(africa)

terra::plot(my.raster, col = my.palette.f(101), range = c(0, 1), plg = list(at = seq(from = 0, to = 1, by = 0.1)), axes = TRUE,
            xlim = terra::ext(africa)[c(1, 2)], ylim = terra::ext(africa)[c(3, 4)])

## Fill with gray
terra::plot(africa, col = "gray80", add = TRUE)

## Overwrite with proper data where available
terra::plot(my.raster, col = my.palette.f(101), range = c(0, 1), add = TRUE)

## Add the relevant borders
if(is.null(serializable.border) == FALSE){
    my.borders <- unwrap(serializable.border)
    plot(my.borders, add = TRUE, col = NA, border = "darkgreen", lwd = 2)
}

@ 

\newpage

\section{Regression with IPV as outcome}
\label{sec:regression-with-ipv}

Start python.

<<python.avoid.startup.bug, echo = FALSE>>=
knitr::opts_chunk$set(python.reticulate=TRUE)
library(reticulate)
reticulate::use_python(path.to.python, required = TRUE)
## use_virtualenv("/mimer/NOBACKUP/groups/globalpoverty1/hans/.my-venv")
reticulate::py_run_string("import pymc as mc")
@ 

Import python modules.

<<python.1, engine='python', engine.path = path.to.python, echo = TRUE, results = "markup">>=
import pandas as pd
import pymc as pm
import pymc.sampling.jax
import numpy as np
import pickle
from patsy import dmatrix
import random
random.seed(9)

import os
os.environ["XLA_PYTHON_CLIENT_ALLOCATOR"] = "platform"

import jax

@

Load the data from a «CSV«-file. % Prepare a slice to print since actual printing works better in R.

<<python.2.load.data, engine='python', warnings = TRUE, results = 'markup', cache = TRUE>>=
df = pd.read_csv("ipv-women2.csv", low_memory=False)
@

\subsection{Data preparation for pymc}
\label{sec:data-prep-pymc}

Create an index starting from zero for each random term, the model will include a random intercept for «country« and for «cluster1«, and save the labels. Here the function «factorize()« from «pandas« is used.

<<spline-data.preparation.random.effects, engine='python', results = 'markup', cache = TRUE>>=
country_index, country_name = df["country"].factorize()
df["country_data"] = df["country"].astype("category").cat.codes

cluster_index, cluster_name = df["cluster1"].factorize()
df["cluster_id_unique_data"] = df["cluster1"].astype("category").cat.codes
@

\subsection{Apply dummy coding for the fixed effects}
\label{sec:apply-dummy-coding}

The «pandas« library in «python« has a function to create dummy variables, «get\_dummies()«. The argument «drop\_first=False« keeps the original columns, which often makes sense since otherwise the first value that happens to appear becomes the reference category. While the choice of reference category is generally not so important, in general make sure to choose a category that do not suffer from complete separation in an interaction term, since that makes the model fitting unnecessary hard. In this particular case, complete separation is not a concern.

%% Iliterate_couple is not yet available

Create a list of specific categorical variables to convert to dummies.

<<spline-data.preparation.dummy.variables, engine='python', cache = TRUE>>=
dummy_vars = ["beat01_couple", "decide2_health", "decide2_purchases", "edudeviation", "v149"]
@

Create dummy variables only for the specified columns.

<<spline-data.preparatation.create.dummy.variables, engine='python', cache = TRUE>>=
df = pd.get_dummies(df, columns=dummy_vars, prefix=dummy_vars, drop_first=False)
@


\subsection{Create the splines}
For this analysis we use «justifyIPV\_cluster« which captures how large proportion of the other women in the neighbourhood thinks that IPV can be legitimate, as the basis for the splines.

If there are missing values in the dataset, drop rows with «NA« in at least one column.

<<spline-data.preparation.drop.na, engine='python', results = 'markup', cache = TRUE>>=
df = df.dropna().reset_index(drop=True)
@

Sort the rows based on «justifyIPV\_cluster«. This is needed because the code that produces the matrix «B« relies on «df« being in that order. We also need to reset the row names, or they will be used when we later concatenate the two objects into one with «pd.concat()«

<<spline-data.preparation.sort, engine='python', results = 'markup', cache = TRUE>>=
df=df.sort_values(by='justifyIPV_cluster',ascending=False).reset_index(drop=True)
@ 

Construct the matrix «B«, and the data frame version of the same data, «spline\_df«. «B« will be used as a model matrix for some of the parameters of the model, while «spline\_df« is convenient to have when we inspect the data visually. In the end, we want to replace «justifyIPV\_cluster« with a version of «B«, and for this purpose, «B\_df« is constructed.

<<spline-data.preparation.spline, engine='python', results = 'markup', cache = TRUE>>=
num_knots = 2
knot_list = np.quantile(df.justifyIPV_cluster, np.linspace(0, 1, num_knots))
@

The splines is generated by the function \href{https://patsy.readthedocs.io/en/latest/spline-regression.html}{«bs()«}, which is part of the «patsy« library.

<<spline-data-preparation-create-B, engine='python', results = 'markup', cache = TRUE>>=
B = dmatrix(
    "bs(justifyIPV_cluster, knots=knots, degree=3, include_intercept=False) - 1",
    {"justifyIPV_cluster": df.justifyIPV_cluster.values, "knots": knot_list[1:-1]},
    )
@ 

Figure \vref{fig:spline-data-preparation-inspect-spline} shows how a given value on «justifyIPV\_cluster« maps to a specific set of weights, without the spline intercept.

<<spline-data-preparation-inspect-spline, engine='python', echo = FALSE, results = 'markup', cache = TRUE, dev = "png", fig.cap = "For a given value on justifyIPV\\_cluster, the values of the curves indicate the weight of each parameter.", fig.width = 4, fig.height = 4, fig.align = "center">>=
import matplotlib.pyplot as plt
spline_df = (
    pd.DataFrame(B)
    .assign(justifyIPV_cluster=df.justifyIPV_cluster.values)
    .melt("justifyIPV_cluster", var_name="spline_i", value_name="value")
)
color = plt.cm.magma(np.linspace(0, 0.80, len(spline_df.spline_i.unique())))
fig = plt.figure()
for i, c in enumerate(color):
    subset = spline_df.query(f"spline_i == {i}")
    subset.plot("justifyIPV_cluster", "value", c=c, ax=plt.gca(), label=i)
plt.legend(title="Spline Index", loc="upper center", fontsize=8, ncol=6);
@

<<spline-data.preparation.create.B_df, engine='python', results = 'markup', cache = TRUE>>=
B_df = pd.DataFrame(B, columns=[f"spline, {i}" for i in range(B.shape[1])])
@

In the final model, «B\_df« will replace «justifyIPV\_cluster«, and while it would be possible to refer to them as they are, it is \emph{convenient} to have single unified data object, especially as we want to predict using R later on. Such a unified data frame is aquired by concatenating «df« and «B\_df«.

<<spline-data.preparation.concatenate.B.and.df, engine='python', results = 'markup', cache = TRUE>>=
df = pd.concat([df, B_df], axis=1)
@

With that in place we can inspect how a given value of «justifyIPV\_cluster« translates to values on the new columns (named ``spline, 0''  to ``spline, 2''), see table \vref{tab:splines}. To produce this slice, only the rows in «df« with unique values on «justifyIPV\_cluster« was kept.

<<spline-create-new-slice, engine='python', cache = TRUE, echo = TRUE, results = "markup">>=
unique_rows = df.drop_duplicates(subset='justifyIPV_cluster').head(6)
my_tab_2 = unique_rows.iloc[:5, np.hstack((df.columns.get_loc("justifyIPV_cluster"), range(len(unique_rows.columns)-(num_knots + 1), len(unique_rows.columns))))]
@

<<spline-print.sorted.table, results='asis', echo = FALSE, cache = TRUE, dependson = "spline-create-new-slice">>=
library(xtable)
print(xtable(py$my_tab_2, caption = "justifyIPV\\_cluster and some of the columns that will replace it in the model. These rows represents the right most part of figure \\vref{fig:spline-data-preparation-inspect-spline}.", digits = 4, label = "tab:splines"), booktabs = TRUE, include.rownames = FALSE)
@

Save the model matrix for predictions in «R«. 

<<python.3, engine='python', cache = TRUE, echo = TRUE, results = "markup">>=
df.to_csv("ipv-women-model_matrix.csv", index=False)
@ 

\subsection{Create the model}
The splines are also ``just ordinary'' $\beta$-coefficients. There is one thing that goes beyond the standard model though, the interaction between «beat01\_couple« and «justifyIPV\_cluster«, and since the latter is substituted by the splines in the model, it is worth spelling out how that is done. Consider table \vref{tab:splines}, which shows the values of «justifyIPV\_cluster« and the splines. Let us define $x_{1}, x_{2}$ and $x_{3}$ as spline 0, spline 1 and spline 2 respectively. The other variable in the interaction, «beat01\_couple«, is also substituted in the model, in this case to dummy variables representing ``Wife justifys IPV'', ``Husband justifys IPV'', ``Both justify IPV'', let us say $x_{4}, x_{5}$ and $x_{6}$. Ignoring the other variables in the model, the variables relevant for «beat01\_couple« and «justifyIPV\_cluster« could be classified into three parts:

\begin{description}
  \item[spline 0] ${x_1} \beta{1}$
  \item[spline 1] ${x_2} \beta{2}$
  \item[spline 2] ${x_3} \beta{3}$
  \item[Only Wife justifys IPV] ${x_4} \beta{4}$
  \item[Only Husband justifys IPV] ${x_5} \beta{5}$
  \item[Both justify IPV] ${x_6} \beta{6}$
  \item[spline 0, Only Wife justifys IPV] $x_{1} x_{4} \beta_{7}$
  \item[spline 1, Only Wife justifys IPV] $x_{2} x_{4} \beta_{8}$
  \item[spline 2, Only Wife justifys IPV] $x_{3} x_{4} \beta_{9}$
    \item[spline 0, Only Husband justifys IPV] $x_{1} x_{5} \beta_{10}$
  \item[spline 1, Only Husband justifys IPV] $x_{2} x_{5} \beta_{11}$
  \item[spline 2, Only Husband justifys IPV] $x_{3} x_{5} \beta_{12}$    
    \item[spline 0, Both justify IPV] $x_{1} x_{6} \beta_{13}$
  \item[spline 1, Both justify IPV] $x_{2} x_{6} \beta_{14}$
  \item[spline 2, Both justify IPV] $x_{3} x_{6} \beta_{15}$    
\end{description}

  Note that the first three rows above, ("spline 0", "spline 1" and "spline 2") will characterise the effect of  «justifyIPV\_cluster« for the couples where "No one justifys IPV", ie the reference category. Similarly, row 4-6 capture the effect of the attitudes of the couple in the scenario where «justifyIPV\_cluster« is zero.

Having this spelled out helps when we defining and naming the priors, however, since we have all the splines data in its own object «B«, we do not manually write down all these terms, we can just instruct «PYMC« to create all the possible combinations of spline and the dummy variables for «beat01\_couple«.

Creating a model in «pymc« involves multiple steps:

\begin{itemize}
\item create the model object
\item add \emph{priors} to the model object
\item add \emph{data} to the model object
\item add a formula that connects the observed outcome via a distribution to the predicted outcome, and produces a likelihood of the data, given the parameters - or rather, in more Bayesian terms, a posterior probability of the parameters, given the data.
\end{itemize}

For models with a large number of variables it is a good idea to deal with them in a systematic way, to make sure nothing is forgotten. Since the model is an object, which we can add parts to in separate steps, we do not need to do everything in one giant ``line'' of code, in fact we even add stuff to the model object in different chunks, making it more readable.

Before actually creating the model object, below defined as «my\_model«, it is a good idea to give human readable names to the parameters of the dummy variables, the levels of the random effects and the splines (for the continuous variables, naming happens naturally when defining the priors.

\subsection{Create the model object}

<<spline-fit-model-1, engine = "python", results = "markup", cache = TRUE>>=
coords={
    "couple_attitude_dims" : ["Husband justifys IPV", "Wife justifys IPV", "Both justify IPV"],
    "education_dims" : ["Incomplete primary", "Complete primary", "Incomplete secondary", "Complete_secondary", "Higher"],
    "decide_health_dims" : ["Health - husband decides", "Health - wife decides", "Healt - joint decisions"],
    "decide_purchase_dims" : ["Purchase - husband decides", "Purchase - wife decides", "Purchase - joint decisions"],
    "edudeviation_dims" : ["Husband has higer edu.", "Wife has higher edu."],
    "literate_couple_dims" : ["Husband iliterate", "Wife iliterate", "Both literate"],
    "1|country_dims" : list(country_name),
    "1|cluster_dims" : list(cluster_name),
    "splines_dims": np.arange(B.shape[1])
}
my_model = pm.Model(coords=coords)
@

\subsection{Add priors to the model object}

Now follows a large number of chunks that taken together define the \emph{priors}, the \emph{data} and the \emph{regression formula}. The priors and the data can be defined in any order. Here we start with the priors, and deal with them one type at a time.

Define the priors, by choosing distribution (in closed form) with the relevant parameteters for each parameter, e.g. the normal distribution and its parameters $\mu$ and $\sigma$. When adding priors for the fixed effects factors, apply dummy coding, one scalar per term, shared sigmas for dummy variables with more than two levels. Start with the intercept.

<<spline-fit-model-2, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    Intercept = pm.Normal("Intercept", mu=0, sigma=5)
@

Now add the three dummy variables, use the names defined in «coords« for «couple\_attitude\_dims«.

<<spline-fit-model-3, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    couple_attitudes = pm.Normal("couple_attitudes", mu=0, sigma=1, dims = "couple_attitude_dims")
@

Priors for the splines, where we use information from B. First, set up parameters for how the splines affect couples in the reference category, ``No one justify IPV''. Note that at this point the user does not need to know how many splines there are, «size=B.shape[1]« creates as many priors as there are splines.

<<spline-fit-model-6, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    spline = pm.Normal("spline", mu=0, sigma=3, size=B.shape[1], dims="splines_dim")
@

In addition, add parameters for the interaction between justifyIPV\_cluster, which is replaced by the splines in the model, and the other categories, ``Wife justifys IPV'', ``Husband justifys IPV'' and ``Both justify IPV''. This make for three separate sets of spline parametres, but since we have defined the coordinates of the model using the «coords« argument of «pymc.Model()«, we do not need to know that, we can just reference the named dimensions «couple\_attitude\_dims« and «splines\_dims«.

<<spline-fit-model-6.5, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    interaction_terms = pm.Normal("interaction_terms", mu=0, sigma=1, dims = ["couple_attitude_dims", "splines_dims"])
@

With priors for the focal variables in place, here is the remaining fixed effects factors
%%     literate_couple = pm.Normal("literate_couple", mu=0, sigma=1, dims = "literate_couple_dims")

<<spline-fit-model-3.5, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    education = pm.Normal("education", mu=0, sigma=1, dims = "education_dims")
    decide_health = pm.Normal("decide_health", mu=0, sigma=1, dims = "decide_health_dims")
    decide_purchase = pm.Normal("decide_purchase", mu=0, sigma=1, dims = "decide_purchase_dims")
    edudeviation = pm.Normal("edudeviation", mu=0, sigma=1, dims = "edudeviation_dims")
@

More fixed effects priors, now the continuous variables.

<<spline-fit-model-4, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    iwi2 = pm.Normal("iwi2", mu=0, sigma=1)
    lnage_wife = pm.Normal("lnage_wife", mu=0, sigma=5)
    ln_numb_of_children = pm.Normal("ln_numb_of_children", mu=0, sigma=2)
    gdi = pm.Normal("gdi", mu=0, sigma=2)
    gni = pm.Normal("gni", mu=0, sigma=2)
@

Priors for the random effects. Every level will get its own dummy variable.

<<spline-fit-model-5, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    random_intercept_country = pm.Normal("1|country", mu=0, sigma=pm.HalfNormal("1|country_sigma", sigma=2.5), dims = "1|country_dims")
    random_intercept_cluster = pm.Normal("1|cluster", mu=0, sigma=pm.HalfNormal("1|cluster_sigma", sigma=2.5), dims = "1|cluster_dims")
@

Now that the priors are defined, inform pymc about where to find the data for the model matrix. Start with the random variables.

<<spline-fit-model-7, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    Z_country = pm.Data("Z_country", df["country_data"])
    Z_cluster = pm.Data("Z_cluster", df["cluster_id_unique_data"])
@

\subsection{Add data to the model object}
And the data for the fixed effects, first the factors.

%%     X_Wife_iliterate = pm.Data("X_Wife_iliterate", df["_Wife iliterate"])
    % X_Husband_iliterate = pm.Data("X_Husband_iliterate", df["edudeviation_Husband iliterate"])
    % X_Both_literate = pm.Data("X_Both_literate", df["_Both literate"])

\scalebox{.75}{\parbox{\textwidth}{

<<spline-fit-model-8, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    X_Husband_justifys_IPV = pm.Data("X_Husband_justifys_IPV", df["beat01_couple_Husband justifys IPV"])
    X_Wife_justifys_IPV = pm.Data("X_Wife_justifys_IPV", df["beat01_couple_Wife justifys IPV"])
    X_Both_justifys_IPV = pm.Data("X_Both_justifys_IPV", df["beat01_couple_Both justify IPV"])
    X_Health_husband_decides = pm.Data("X_Health_husband_decides", df["decide2_health_husband decides"])
    X_Health_wife_decides = pm.Data("X_Health_wife_decides", df["decide2_health_wife decides"])
    X_Health_joint_decisions = pm.Data("X_Health_joint_decisions", df["decide2_health_joint decisions"])
    X_Purchases_husband_decides = pm.Data("X_Purchases_husband_decides", df["decide2_purchases_husband decides"])
    X_Purchases_wife_decides = pm.Data("X_Purchases_wife_decides", df["decide2_purchases_wife decides"])
    X_Purchases_joint_decisions = pm.Data("X_Purchases_joint_decisions", df["decide2_purchases_joint decisions"])
    X_Husband_higher_edu = pm.Data("X_Husband_higher_edu", df["edudeviation_Husband has higher edu."])
    X_Wife_higher_edu = pm.Data("X_Wife_higher_edu", df["edudeviation_Wife has higher edu."])
    X_incomplete_primary = pm.Data("X_incomplete_primary", df["v149_Incomplete primary"])
    X_complete_primary = pm.Data("X_complete_primary", df["v149_Complete primary"])
    X_incomplete_secondary = pm.Data("X_incomplete_secondary", df["v149_Incomplete secondary"])
    X_complete_secondary = pm.Data("X_complete_secondary", df["v149_Complete secondary"])
    X_higher = pm.Data("X_higher", df["v149_Higher"])
@

}}

Then the continuous variables, note that there is no reference to the focal variable «justifyIPV\_cluster«, instead we will use a version of «B«.

<<spline-fit-model-9, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    X_iwi = pm.Data("X_iwi", df["iwi2"])
    X_lnage_wife = pm.Data("X_lnage_wife", df["lnage_wife"])
    X_ln_numb_of_children = pm.Data("X_ln_numb_of_children", df["ln_numb_of_children"])
    X_gdi = pm.Data("X_gdi", df["gdi_s"])
    X_gni = pm.Data("X_gni", df["gni_s"])
@

\subsection{Formulate the regression equation}
Next step is to add a formula that connects the observed outcome via a distribution to the data and the priors.

%% TODO add
      % X_Husband_iliterate * literate_couple[0] + \
      % X_Wife_iliterate * literate_couple[1] + \
      % X_Both_literate * literate_couple[2] + \

<<spline-fit-model-10, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    theta_as_logit = Intercept + \
      X_Husband_justifys_IPV * couple_attitudes[0] + \
      X_Wife_justifys_IPV * couple_attitudes[1] + \
      X_Both_justifys_IPV * couple_attitudes[2] + \
      X_Health_husband_decides * decide_health[0] + \
      X_Health_wife_decides * decide_health[1] + \
      X_Health_joint_decisions * decide_health[2] + \
      X_Purchases_husband_decides * decide_purchase[0] + \
      X_Purchases_wife_decides * decide_purchase[1] + \
      X_Purchases_joint_decisions * decide_purchase[2] + \
      X_Husband_higher_edu * edudeviation[0] + \
      X_Wife_higher_edu * edudeviation[1] + \
      X_incomplete_primary * education[0] + \
      X_complete_primary * education[1] + \
      X_incomplete_secondary * education[2] + \
      X_complete_secondary * education[3] + \
      X_higher * education[4] + \
      X_iwi * iwi2 + \
      X_ln_numb_of_children * ln_numb_of_children + \
      X_lnage_wife * lnage_wife + \
      X_gdi * gdi + \
      X_gni * gni + \
      random_intercept_country[Z_country] + \
      random_intercept_cluster[Z_cluster] + \
      pm.math.dot(np.asarray(B, order="F"), spline.T) + \
      pm.math.dot(np.asarray(B, order="F"), interaction_terms[0].T) * X_Husband_justifys_IPV + \
      pm.math.dot(np.asarray(B, order="F"), interaction_terms[1].T) * X_Wife_justifys_IPV + \
      pm.math.dot(np.asarray(B, order="F"), interaction_terms[2].T) * X_Both_justifys_IPV
@

Make sure to use the correct data part (e.g. «X\_Husband\_justifys\_IPV«) together with the relevant named parameter «interaction\_terms[0]«, ie, make sure the zero-th index of the dimensions used in «interaction\_terms« (``Only husband justifys IPV'', see «couple\_attitude\_dims«) corresponds to the same level as the data part, or the resulting parameter estimates will turn up under the wrong name.

\subsection{Choose a distribution for the outcome}
Make it a logistic regression by tying the observed value to the model via the Bernoulli distribution.
<<spline-fit-model-11, engine = "python", results = "markup", cache = TRUE>>=
with my_model:
    theta_as_probability = pm.invlogit(theta_as_logit)
    y = pm.Data("y", df["viol01_wife"] == True)
    Y_obs = pm.Bernoulli("Y_obs", p=theta_as_probability, observed=y)
@

\subsection{Start sampling parameter values}
To be able to post process the parameter samples, these are saved  to disk with the function «dump()« in the library «pickle«.

% Seems like import pymc.sampling.jax works on system that does not have my custom version of pymc.
% jax works even if no GPU is available

<<spline-fit-model-12, engine = "python", results = "markup", eval = !file.exists("ipv-women2.pydata"), cache = TRUE>>=
import pickle
import pymc.sampling.jax
with my_model:
    my_fit_pymc = pm.sampling.jax.sample_numpyro_nuts(random_seed=1234, tune=100, draws=400, target_accept=0.8, chains = 4, chain_method='parallel', progressbar = False, idata_kwargs=dict(log_likelihood=False), postprocessing_backend="cpu")
pickle.dump(my_fit_pymc, open('ipv-women2.pydata', 'wb'))
@

\section{Evaluate the model fit}
\label{sec:evaluate-model}

Inspect the r-hat statistics, and the High Density Intervals for the fixed effects parameters, see table \vref{tab:convergence}.

<<spline-evaluate-model-2, engine = "python", results = "markup", cache = TRUE, eval = !file.exists("spline-evaluate-model-3-summary.csv")>>=
import pickle
my_fit_pymc = pickle.load(open("ipv-women2.pydata", "rb"))
import arviz
summary_df = arviz.summary(my_fit_pymc, var_names=["~country", "~cluster"], filter_vars="like")[["hdi_3%", "hdi_97%","r_hat"]]
summary_df.to_csv("spline-evaluate-model-3-summary.csv", index=True)
@

\begin{footnotesize}
<<spline-pretty.print.convergence, results="asis", echo = FALSE>>=
conv.tab <- read.csv("spline-evaluate-model-3-summary.csv")
colnames(conv.tab) <- c("Parameter", "HDI_3%", "HDI_97%","r_hat")
print(xtable(conv.tab, caption = "High Density Interval (HDI) and convergence statistics for fixed effect parameters.", label = "tab:convergence"), include.rownames = FALSE, booktabs = TRUE)
@ 
\end{footnotesize}

Show a trace plot of «couple\_attitudes«, note that these parameters only describe the effect of the attitudes in question when «justifyIPV\_cluster« is at zero, and also note that the reference category is ``No one justifys IVP''. This graph indicates that the model had enough tuning (or ``burn-in'') since the early samples are at the same level as the last samples; and the large fluctuations between nearby samples indicate that the parameter space is searched thoroughly, see figure \vref{fig:spline-evaluate-model-5}.

<<spline-evaluate-model-5, engine = "python", dev = "png", cache = TRUE, fig.cap = "Density plot and trace plot for the attitude of the couple (the scale here is logit).">>=
import pickle
import arviz
import matplotlib.pyplot as plt

my_fit_pymc = pickle.load(open("ipv-women2.pydata", "rb"))
axes = arviz.plot_trace(my_fit_pymc, var_names = ["couple_attitudes"], compact = True)
labels = ["Husband justifies IPV", "Wife justifies IPV", "Both justify IPV"]
k = (len(axes[0, 0].get_lines()) // 3)
for ax in axes.flatten():
    lines = ax.get_lines()
    legend_lines = [lines[0], lines[0 + k], lines[0 + 2 * k]]
    ax.legend(legend_lines, labels, loc="upper right")

plt.show()

@

\section{Predict and visualise the effects}
\subsection{Export parameters needed for predictions}
<<spline-export-params, engine = "python", results = "markup", cache = TRUE, eval = !file.exists("ipv-women-sampled-parameters.csv")>>=
import arviz
import pickle
my_fit_pymc = pickle.load(open("ipv-women2.pydata", "rb"))
df_results = my_fit_pymc.to_dataframe(groups = "posterior", include_index=False)
df_results.drop(
    columns=[col for col in df_results.columns if "|" in str(col) or col in {"chain", "draw"}], 
    inplace=True
)
df_results.to_csv("ipv-women-sampled-parameters.csv", index=False)
@

\subsection{Import parameters into R}
Import both the sampled parameters and the ``model matrix'' and make a clean version of the model matrix that \emph{only} contains the data that corresponds to the columns in params, and is in the \emph{right order}. The column order in the set of sampled parameters is alphabetic, except for the Intercept which comes first.

\begin{compactenum}
\item Intercept
\item couple\_attitudes
\item spline
\item interaction\_terms
\item decide\_health
\item decide\_purchase
\item education
\item edudeviation
\item iwi2
\item ln\_numb\_of\_children  
\item lnage\_wife
\item gdi
\item gni
\item random term cluster (ignore when predicting)
\item random term country (ignore when predicting)
\end{compactenum}

<<spline-clean.mm>>=
ipv.mm <- read.csv("ipv-women-model_matrix.csv", check.names = FALSE)
params <- read.csv("ipv-women-sampled-parameters.csv", check.names = FALSE)

my.recodes <- "'True' = 1; 'False' = 0"
attitudes <- c("beat01_couple_Both justify IPV", "beat01_couple_Husband justifys IPV", "beat01_couple_Wife justifys IPV")

couple_attitudes <- cbind(sapply(c(attitudes), function(x) {
    car::recode(ipv.mm[[x]], recodes = my.recodes)                            
}, simplify = TRUE))

## dynamically adjust to any number of splines
splines.iterator <- 0:py$num_knots
my.spline.names <- paste0("spline, ", splines.iterator)
interaction_terms <- unlist(sapply(c(attitudes), function(attitude) {
    sapply(my.spline.names, function(spline){
        return(car::recode(ipv.mm[[attitude]], recodes = my.recodes) *
            ipv.mm[[spline]])
    }, simplify = FALSE)
}, simplify = FALSE), recursive = FALSE)

dummies <- cbind(sapply(c(
    "decide2_health_joint decisions", "decide2_health_husband decides", "decide2_health_wife decides",
    "decide2_purchases_husband decides", "decide2_purchases_joint decisions", "decide2_purchases_wife decides",
    "v149_Complete primary", "v149_Complete secondary", "v149_Higher", "v149_Incomplete primary", "v149_Incomplete secondary",    
    "edudeviation_Husband has higher edu.", "edudeviation_Wife has higher edu."), function (dummy){
        car::recode(ipv.mm[[dummy]], recodes = my.recodes)        
    }))

splines <- ipv.mm[my.spline.names]

as.is.1 <- ipv.mm[c("gdi_s", "gni_s")]
as.is.2 <- ipv.mm[c("iwi2", "ln_numb_of_children", "lnage_wife")]

my.mm <- cbind(Intercept = rep(1, nrow(ipv.mm)), couple_attitudes, dummies, as.is.1, interaction_terms, as.is.2, splines)

## get the sample means
my.means <- apply(my.mm, 2, mean)

params.matrix <- as.matrix(params)
mean.matrix <- as.matrix(my.means)
@

\subsection{Construct the relevant model matrix}
\label{sec:constr-relev-model}
Build the grid of values at which to generate predicted probabilities for each level of the factor variable that captures the attitudes of the couple. Even if splines have replaced the variable «justifyIPV\_cluster«, proceed as normal by finding relevant values on «justifyIPV\_cluster«, e.g. all non-extreme half-deciles, and for each interesting value on «justifyIPV\_cluster« find the corresponding set of values by extracting the values for the splines for cases that have the decile value on «justifyIPV\_cluster«. This is why «justifyIPV\_cluster« was retained in the model matrix even if it was not used when the model was fit to data.

<<spline-find-interesting.points>>=
quantiles <- quantile(ipv.mm$justifyIPV_cluster, probs = seq(from = 0,  to = 1, by = 0.01))

## Find the observed value closest to the quantile
these <- sapply(quantiles, function(x) { which.min(abs(ipv.mm$justifyIPV_cluster - x)) })
interesting.points.temp <- ipv.mm[these, c("justifyIPV_cluster", my.spline.names)]
keep.these <- which(duplicated(interesting.points.temp) == FALSE)
interesting.points <- interesting.points.temp[keep.these, ]
quantiles <- quantiles[keep.these]
@

<<spline-show-interesting.points, echo = FALSE, results="asis", eval = FALSE>>=
print(xtable(interesting.points, label = "tab:half-deciles", caption = "The non extreme half-deciles of «justifyIPV\\_cluster« and the corresponding values on the splines."), include.rownames = FALSE, booktabs = TRUE)
@

Build one small model matrix for each level of the factor variable.
\begin{itemize}
\item Start with a copy of the mean matrix, replicated as many times as there are rows in the  object «quantiles«.
\item Copy the splines data from the object «interesting.points«.
\item Set all dummies to 0.
\item Loop over the possible values on the factor variable, including the reference category  
  \begin{itemize}
  \item Change the dummy for the current level to 1.
  \item Create the data for the interaction by multiplying the dummies and the splines data.
  \end{itemize}
\end{itemize}

<<spline-mm-to-predict-for>>=
mm.by.attitude.f <- function(this.attitude){
    ## Duplicate the mean matrix
    mean.matrix.replicated <- do.call(rbind, replicate(length(quantiles), t(mean.matrix), simplify = FALSE))
    ## Copy the values of splines
    mean.matrix.replicated[, my.spline.names] <- as.matrix(interesting.points[, my.spline.names])
    ## Set the dummies of the focal variable to 0 or 1
    for(dummy.var in attitudes){
        if(dummy.var == this.attitude){
            mean.matrix.replicated[, dummy.var] <- rep(1, length(quantiles))
        } else {
            mean.matrix.replicated[, dummy.var] <- rep(0, length(quantiles))
        }
    }
    ## Interaction terms
    interaction_terms <- unlist(sapply(c(attitudes), function(attitude) {
        sapply(my.spline.names, function(spline){
            return(mean.matrix.replicated[, attitude] * mean.matrix.replicated[, spline])
        }, simplify = FALSE)
    }, simplify = FALSE), recursive = FALSE)
    ## overwrite values in mean.matrix.replicated with the newly calculated values
    for(interaction.term in names(interaction_terms)){
        mean.matrix.replicated[, interaction.term] <- interaction_terms[[interaction.term]]
    }
    return(mean.matrix.replicated)
}
mms <- sapply(c("None justifys IPV", attitudes), mm.by.attitude.f, simplify = FALSE)
@

\subsection{Predict}
\label{sec:predict}

Generate the predicted probabilities.

<<spline-predict>>=
my.probs <- c(0.025, 0.5, 0.975)
results <- sapply(mms, function(my.mm){
    apply(plogis(params.matrix %*% t(my.mm)), 2, quantile, probs = my.probs)
}, simplify = FALSE)
@

\subsection{Visualise the effects}
\label{sec:visualise-effects}

Plot the predicted probabilities.

<<spline-plot-predictions, fig.cap = "Predicted probabilities for a woman to be exposed to IPV conditioned on the attitudes in the couple and the attitudes of the other women in the local area.", fig.width = 6, fig.align = "center">>=
par(mar = c(4, 5, 5, 13))
plot(y = results[["None justifys IPV"]]["50%",],
     x = interesting.points[["justifyIPV_cluster"]],
     xlab = "Neighbourhood justification of IPV",
     ylab = "Probability of IPV",
     type = "l", col = "red", ylim = c(min(unlist(results)), max(unlist(results))),
     lwd = 3)
grid()
my.cols <- c("red", "darkgreen", "lightgreen", "orange")
my.alpha.cols <- sapply(my.cols, transparent, alpha = 0.6)
my.names <- c("None justifys IPV", attitudes)
for(i in 1:length(my.names)){
    polygon(y = c(results[[my.names[i]]]["2.5%",], rev(results[[my.names[i]]]["97.5%",])),
            x = c(interesting.points[["justifyIPV_cluster"]], rev(interesting.points[["justifyIPV_cluster"]])), col = my.alpha.cols[i], border = NA)
    lines(y = results[[my.names[i]]]["50%",],
          x = interesting.points[["justifyIPV_cluster"]],
          col = my.cols[i], lwd = 3, lty = 1)
}

legend("right",
       title = "Justifications in the couple",
       legend = c("No one justifies IPV", "Husband justifies IPV", 
                  "Wife justifies IPV", "Wife & husband justify IPV"), 
       col = c("red", "lightgreen", "orange", "darkgreen"), 
       lwd = 3, 
       lty = 1, 
       bty = "n", 
       ncol = 1,
       inset = c(-1.1, 0),
       xpd = TRUE
)

@



\bibliographystyle{apacite}
\bibliography{references}

\end{document}
